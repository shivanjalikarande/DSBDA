{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9474927-630f-4111-8ee2-323f77e428d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0m\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/pict/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/pict/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f744c-1d3c-44dd-85c8-bc89ef2c4911",
   "metadata": {},
   "source": [
    "\n",
    "**NLTK (Natural Language Toolkit)** is a powerful Python library used for Natural Language Processing (NLP). \n",
    "- It provides tools to work with human language data, including text processing, tokenization, stemming, lemmatization, part-of-speech tagging, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f555452-96ea-42f6-ac33-877d7bd92d5e",
   "metadata": {},
   "source": [
    "## **Algorithm for Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f0f6c-c5dc-4605-8c9e-b4ea1be23ded",
   "metadata": {},
   "source": [
    "#### ***1. Download Require Packages***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb7ddc8d-7ad3-472e-8c00-397643d3d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pict/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/pict/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pict/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/pict/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pict/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.Download require packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c4725-b894-425c-a73a-6c45b691243c",
   "metadata": {},
   "source": [
    "#### ***2. Initialize Text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2431b92d-f582-4748-ad88-1e09ff2449dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Initialize Text\n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18627b-2827-4cf3-87a7-23cddfbe1d67",
   "metadata": {},
   "source": [
    "#### ***3. Perform Tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1c65570-b8c6-4898-899b-8fbdecb1b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "#3.Perform tokenization\n",
    "#a. Sentence Tokenization - Sentence tokenization splits a given text into individual sentences.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8957fe4e-dada-442e-8f2b-07137052a083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#b. Word Tokenization - Word tokenization splits the text into individual words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f658918-471d-45ac-9cbc-a932d8ffb008",
   "metadata": {},
   "source": [
    "#### ***4. Removing Punctuations and Stop Word***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b136345a-b6f7-4a59-bd64-dd630b3e62b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a. Print english stop words\n",
    "from nltk.corpus import stopwords \n",
    "stop_words=set(stopwords.words(\"english\"))   #loads the list of common English stop words. set() improves lookup speed for removal operations.\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9128454-b601-4bb2-9a57-4328aff1950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to remove stop words with NLTK library in Python \n",
      "Tokens:  ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n"
     ]
    }
   ],
   "source": [
    "text= \"How to remove stop words with NLTK library in Python?\"\n",
    "\n",
    "import re    #regular expressions module in Python.\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)  #removing all non-alphabetic characters (anything that is not a letter from a to z or A to Z) and replacing them with spaces.\n",
    "print(text)\n",
    "\n",
    "#4b.# Tokenizing words\n",
    "tokens = word_tokenize(text.lower())\n",
    "print('Tokens: ',tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b74d370-b18f-43f7-863f-d1a767b42da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "#4c.Removing stop words\n",
    "\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "        \n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18da4c-7d07-4074-95ff-ef6fca0e45c3",
   "metadata": {},
   "source": [
    "#### ***5. Perform Stemming***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14300655-ffea-4a09-9a73-60e66e66aadc",
   "metadata": {},
   "source": [
    "Stemming - a technique in NLP that reduces words to their root (or base) form\n",
    "Stemming removes suffixes from words to get their root form.\n",
    "Use in -> Sentiment Analysis, Search Engines (Information Retrieval), Text Classification (Spam Filtering, News Categorization), Chatbots and AI Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "255118af-1084-4110-8332-3d0ff469e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n",
      "waiter\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  #nltk.stem → A module in nltk(python library) that contains stemming algorithms.\n",
    "\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\",\"waiter\",\"studies\"]\n",
    "\n",
    "ps = PorterStemmer()  # Create a PorterStemmer object\n",
    "\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)\n",
    "\n",
    "#\"waiter\" is not reduced to \"wait\" because it's a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70ebe3-9ac3-47f0-bf4b-35b0f5ce7862",
   "metadata": {},
   "source": [
    "#### ***6. Perform Lemmatization***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224da68-4e6d-4ab9-ae2e-6285cc976e34",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique in Natural Language Processing (NLP) that reduces words to their base or dictionary form (lemma), considering the word's meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1843b2d-fa76-4cd6-95cf-08b962b92466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studing is studing\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n",
      "Lemma for studing is study\n"
     ]
    }
   ],
   "source": [
    "#Perform Lemmatization \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studing cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
    "\n",
    "print(\"Lemma for studing is\",wordnet_lemmatizer.lemmatize(\"studying\", pos=\"v\"))  #mentioned parts of speech as 'verb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570cd22-e626-40b1-83ec-2c28d00afde3",
   "metadata": {},
   "source": [
    "By default, WordNetLemmatizer assumes every word is a noun unless specified otherwise.\n",
    "\n",
    "\"studies\" (noun) → lemmatized to \"study\" (correct).\n",
    "\n",
    "\"studying\" (verb) → not lemmatized unless explicitly given pos=\"v\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f521d88-8af5-47d3-9468-90a6a9137398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for 'running' (v) → run\n",
      "Lemma for 'flies' (n) → fly\n",
      "Lemma for 'better' (a) → good\n",
      "Lemma for 'mice' (n) → mouse\n",
      "Lemma for 'ate' (v) → eat\n",
      "Lemma for 'happily' (r) → happily\n",
      "Lemma for 'quickly' (r) → quickly\n"
     ]
    }
   ],
   "source": [
    "#More Examples of lemmatization\n",
    "words = [\"running\", \"flies\", \"better\", \"mice\", \"ate\", \"happily\", \"quickly\"]\n",
    "pos_tags = [\"v\", \"n\", \"a\", \"n\", \"v\", \"r\", \"r\"]  # Assigning POS manually\n",
    "\n",
    "for word, pos in zip(words, pos_tags):\n",
    "    print(f\"Lemma for '{word}' ({pos}) → {wordnet_lemmatizer.lemmatize(word, pos=pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225aa2e-436f-470c-a4ca-1da924be75a2",
   "metadata": {},
   "source": [
    "#### ***7. Apply POS Tagging to text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b227c0c4-23b4-49c1-9e91-2e0275ccc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize words:  ['The', 'pink', 'sweater', 'fit', 'her', 'perfectly']\n",
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/pict/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "text = \"The pink sweater fit her perfectly\"\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenize words: \",words)\n",
    "\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba60dbb-fcdc-4c42-bb2e-fd5b12adb291",
   "metadata": {},
   "source": [
    "The `nltk.pos_tag()` function in NLTK (Natural Language Toolkit) is used to assign Part-of-Speech (POS) tags to words in a sentence.  \n",
    "POS tagging involves identifying the grammatical category of each word in a sentence, such as:  \n",
    "\n",
    "- **Noun (NN)** → \"cat,\" \"table\"  \n",
    "- **Verb (VB)** → \"run,\" \"write\"  \n",
    "- **Adjective (JJ)** → \"beautiful,\" \"fast\"  \n",
    "- **Adverb (RB)** → \"quickly,\" \"very\"  \n",
    "- **Pronoun (PRP)** → \"he,\" \"they\"  \n",
    "- **Preposition (IN)** → \"in,\" \"on\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f8b78-c071-4c59-bbb0-cd5dafa74ba3",
   "metadata": {},
   "source": [
    "## **Algorithm for Create representation of document by calculating TFIDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087eb9d-abdb-4f0f-aece-b387f2a9c538",
   "metadata": {},
   "source": [
    "- **TF (Term Frequency)** measures how frequently a word appears in a document.\n",
    "- **IDF (Inverse Document Frequency)** penalizes common words across multiple documents.\n",
    "- **TF-IDF** is a weighted score indicating how important a word is to a specific document in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c2481-2b4f-4816-b3f8-79a54a5101c1",
   "metadata": {},
   "source": [
    "#### ***1. Import the necessary libraries.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c70b2d6-b0aa-487e-a6e1-a7408311fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer   # A scikit-learn class that automates TF-IDF computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac6ef9-ef03-4362-bf2f-c497a9f35d2a",
   "metadata": {},
   "source": [
    "#### ***2. Initialize the Documents.***\n",
    "To calculate the TF-IDF values for words in these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69198e1f-8097-4f93-afe2-e02e22cc52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc2ce-631b-40a7-a64c-732fc811701d",
   "metadata": {},
   "source": [
    "#### ***3. Create BagofWords (BoW) for Document A and B. - Tokenization***\n",
    "Splitting sentences into words using ' ' (seperated by space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "560e361a-7435-48c2-84e1-56a5025bcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3031d0e-b81d-4542-a025-75a9581a3ffb",
   "metadata": {},
   "source": [
    "#### ***4. Create Collection of Unique words from Document A and B.***\n",
    "To get Union of words from both documents to form the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9122df7-ddb4-44ff-82b2-8176efb274d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jupiter', 'from', 'planet', 'is', 'the', 'fourth', 'largest', 'Sun', 'Planet', 'Mars'}\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a4c22-24ee-4e14-b442-5d2c7f2e1cb3",
   "metadata": {},
   "source": [
    "#### ***5. Create a dictionary of words and their occurrence for each document in the corpus***\n",
    "We initialize dictionaries where:\n",
    "\n",
    "- **numOfWordsA**: Stores the frequency of each word in **Document A**  \n",
    "- **numOfWordsB**: Stores the frequency of each word in **Document B**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d17bae8b-d911-42e8-992e-0fb2d426ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numOfWordsA:  {'Jupiter': 1, 'from': 0, 'planet': 0, 'is': 1, 'the': 1, 'fourth': 0, 'largest': 1, 'Sun': 0, 'Planet': 1, 'Mars': 0}\n",
      "numOfWordsB:  {'Jupiter': 0, 'from': 1, 'planet': 1, 'is': 1, 'the': 2, 'fourth': 1, 'largest': 0, 'Sun': 1, 'Planet': 0, 'Mars': 1}\n"
     ]
    }
   ],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "    \n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "\n",
    "print(\"numOfWordsA: \",numOfWordsA)\n",
    "print(\"numOfWordsB: \",numOfWordsB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538fab1-30c3-415d-a5cb-30efcebfa199",
   "metadata": {},
   "source": [
    "#### ***6. Compute the term frequency for each of our documents.***\n",
    "\n",
    "##### Formula for Term Frequency (TF):\n",
    "\n",
    "**TF** = (Number of times word appears in a document) / (Total words in the document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "50f4aa74-3e9c-4a5e-8ad1-6f40a97f3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfA:  {'Jupiter': 0.2, 'from': 0.0, 'planet': 0.0, 'is': 0.2, 'the': 0.2, 'fourth': 0.0, 'largest': 0.2, 'Sun': 0.0, 'Planet': 0.2, 'Mars': 0.0}\n",
      "tfB:  {'Jupiter': 0.0, 'from': 0.125, 'planet': 0.125, 'is': 0.125, 'the': 0.25, 'fourth': 0.125, 'largest': 0.0, 'Sun': 0.125, 'Planet': 0.0, 'Mars': 0.125}\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)  # Total words in the document\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)  # TF Formula\n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "\n",
    "print(\"tfA: \",tfA)\n",
    "print(\"tfB: \",tfB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c2988-8234-4b7d-bf68-4c0d703a2dad",
   "metadata": {},
   "source": [
    "#### ***7. Compute the term Inverse Document Frequency.***\n",
    "\n",
    "- To evaluate how important a word is in a document relative to a collection of documents (corpus).\n",
    "- IDF helps reduce the weight of common words that appear in many documents and increase the importance of rare words that appear in fewer documents.\n",
    "\n",
    "\n",
    "##### Formula for Inverse Document Frequency (IDF):\n",
    "\n",
    "**IDF** = log (Total number of documents / Number of documents containing the word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1094ab8-70cf-4bab-bced-25a1e1dcb215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'the': 0.0,\n",
       " 'fourth': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'Mars': 0.6931471805599453}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(documents):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:  # Word appears in the document\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))  # IDF Formula\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d9354-fc25-40f4-919c-75c424181aa3",
   "metadata": {},
   "source": [
    "#### ***8. Compute the term TF/IDF for all words.***\n",
    "\n",
    "##### Formula for TF-IDF:\n",
    "\n",
    "TF−IDF=TF×IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb8a90a3-ae40-4f26-9ad8-c9cefa3ea233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Jupiter      from    planet   is  the    fourth   largest       Sun  \\\n",
      "0  0.138629  0.000000  0.000000  0.0  0.0  0.000000  0.138629  0.000000   \n",
      "1  0.000000  0.086643  0.086643  0.0  0.0  0.086643  0.000000  0.086643   \n",
      "\n",
      "     Planet      Mars  \n",
      "0  0.138629  0.000000  \n",
      "1  0.000000  0.086643  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]  # TF-IDF Formula\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c3aee-9579-4aba-9c7a-03cb9cb755e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
